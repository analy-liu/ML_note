 # 数据分析全生命周期对应技术与方法
- [数据分析全生命周期对应技术与方法](#数据分析全生命周期对应技术与方法)
  - [1. 数据获取](#1-数据获取)
    - [1.1. 技术需求](#11-技术需求)
    - [1.2. 方法](#12-方法)
      - [1.2.1. 内部来源](#121-内部来源)
      - [1.2.2. 外部来源](#122-外部来源)
      - [1.2.3. 埋点](#123-埋点)
  - [2. 数据预处理](#2-数据预处理)
    - [2.1. 技术需求](#21-技术需求)
    - [2.2. 方法](#22-方法)
      - [2.2.1. 结构化与非机构化数据](#221-结构化与非机构化数据)
      - [2.2.2. 数据清洗(data cleaning)](#222-数据清洗data-cleaning)
      - [2.2.3. 特征工程](#223-特征工程)
  - [3. 可视化](#3-可视化)
    - [3.1. 技术需求](#31-技术需求)
  - [4. 分析与建模](#4-分析与建模)
    - [4.1. 技术需求](#41-技术需求)
  - [参考文献](#参考文献)

数据分析整个流程大致如下：
1. 分析需求：
   分析项目的目的和需求，了解这个项目属于什么问题，要达到什么效果。
2. 数据获取：
   数据一般有几种获取方式：数据库提取、爬虫收集、提前埋点
3. 数据清洗
   获取数据之后，做基本的数据清洗。
4. 特征工程：
   这个需要耗费很大的精力，如果特征工程做的好，那么，后面选择什么算法其实差异不大，反之，不管选择什么算法，效果都不会有突破性的提高。
5. 选择算法：
   可以把所有能跑的算法先跑一遍，看看效果，分析模型评价指标
   模型评价指标:
   1. 精准率(precesion)与召回率(recall)
   2. 准确率(accuracy)和错误率(errorrate)
   3. f1-score
   4. ROC曲线、AUC
   看看有没有什么异常（比如有好几个算法precision特别好，但是recall特别低，这就要从数据中找原因，或者从算法中看是不是因为算法不适合这个数据），如果没有异常，那么就进行下一步，
6. 算法调优：
   选择一两个跑的结果最好的算法进行调优。调优的方法很多，调整参数的话可以用网格搜索、随机搜索等，调整性能的话，可以根据具体的数据和场景进行具体分析。
   调优后再跑一边算法，看结果有没有提高，如果没有，找原因，
      1. 特征问题就回到第四步再进行特征工程；
      2. 数据质量问题就回到第三步看数据清洗有没有遗漏，异常值是否影响了算法的结果；
      3. 算法问题就回到第五步，看算法流程中哪一步出了问题。
   如果实在不行，可以搜一下相关的论文，看看论文中有没有解决方法。这样反复来几遍，就可以出结果了，
7. 最终结果：
   写技术文档和分析报告，再向业务人员或产品讲解我们做的东西，然后他们再提建议/改需求，不断循环，最后代码上线，改bug，直到结项。

## 1. 数据获取
### 1.1. 技术需求
   1. SQL
   2. excel
   3. 爬虫:python(requests、lxml、selenium等包)

### 1.2. 方法
   在数据分析之前首先需要进行数据获取，首先要界定分析范围和数据来源，保证分析过程合理有效。数据分析可分为内部来源和外部来源
#### 1.2.1. 内部来源
   1. 企业内部数据库
   2. 机器、传感器数据
   3. 问卷调查
   4. 埋点
#### 1.2.2. 外部来源
   1. 互联网公开信息
   2. 付费数据
   3. 网络采集

#### 1.2.3. 埋点
是在应用中特定的流程收集一些信息，用来跟踪应用使用的状况，后续用来进一步优化产品或是提供运营的数据支撑
2. 埋点方法
   1. 自己公司研发在产品中注入代码统计，并搭建起相应的后台查询。
   2. 第三方统计工具，如友盟、神策、Talkingdata、GrowingIO等。
3. 常用埋点
   1. 访问次数（Visits）与访问人数（Vistors）
   2. 停留时长
   3. 跳出率：进入网站后没有其他操作就退出
   4. 退出率：进入网站后还浏览了其他内容后退出
   5. 转化率

## 2. 数据预处理
### 2.1. 技术需求
1. python(pandas、numpy、matplotlib.pyplot等包)
2. excel
### 2.2. 方法
数据预处理可以称为建立机器学习模型最重要的一步，数据预处理没做好，建立的模型就没有意义或者无效。
#### 2.2.1. 结构化与非机构化数据
数据大致分为非结构化数据和结构化数据。 
结构化数据是以特定格式提供的信息，因此易于阅读。NoSQL 数据库中的表，变量作为列，记录作为行或者键值对就是一个结构化数据的例子。   
非结构化数据是大部分为格式自由、数据类型不一致的数据。医生的手写处方和从博客中收集的影评是两个非结构化数据的例子。  

1. 结构化数据处理
   1. 数据清洗：去除唯一属性、缺失值、重复值、异常值处理、数据标准化等
   2. 特征工程：特征选择、特征编码、降维、增维等
2. 非结构化数据处理
   1. Web页面信息内容提取；
   2. 结构化处理（含文文本的词汇切分、词性分析、歧义处理等）；
   3. 语义处理（含实体提取、词汇相关度、句子相关度、篇章相关度、句法分析等）
   4. 文本建模（含向量空间模型、主题模型等）
   5. 隐私保护（含社交网络的连接型数据处理、位置轨迹型数据处理等）
#### 2.2.2. 数据清洗(data cleaning)
数据清洗就是对原始数据通过丢弃、填充、替换、去重等操作，实现去除异常、纠正错误、补足缺失的目的。  
在数据清洗过程中，主要处理的是缺失值、异常值和重复值。
#### 2.2.3. 特征工程
特征工程就是**对原始数据进行一系列工程处理，将其提炼为特征，作为输入供算法和模型使用**。

## 3. 可视化
### 3.1. 技术需求
1. python
   1. 基础：matplotlib
   2. 统计：Seaborn
   3. 交互：Pyecharts、Bokeh、Plotly
   4. 地图：Mapbox、Geoplotlib
2. Tableau
3. Power BI
4. excel数据透视图

## 4. 分析与建模
### 4.1. 技术需求
1. python(sklearn、sqlalchemy等包)
2. tensorflow(或pytorch)等深度学习框架
3. 基础的机器学习算法
4. 深度学习算法
5. NLP

## 参考文献
https://www.cnblogs.com/charlotte77/p/5606926.html