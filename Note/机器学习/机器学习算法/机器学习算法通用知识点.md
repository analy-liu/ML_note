 # 机器学习算法通用知识点
- [机器学习算法通用知识点](#机器学习算法通用知识点)
  - [1. 监督学习、非监督学习、强化学习](#1-监督学习非监督学习强化学习)
    - [1.1. 监督学习](#11-监督学习)
    - [1.2. 无监督学习](#12-无监督学习)
  - [2. 梯度下降](#2-梯度下降)
  - [3. 正则项](#3-正则项)
  - [4. 损失函数](#4-损失函数)
  - [5. 参数与非参数模型](#5-参数与非参数模型)
## 1. 监督学习、非监督学习、强化学习
### 1.1. 监督学习
**监督学习是一种目的明确的训练方式，你知道得到的是什么**
监督并不是指人站在机器旁边看机器做的对不对，而是下面的流程：

1. 选择一个适合目标任务的机器学习模型
2. 把训练集给机器去学习（监督学习需要给数据打标签）
3. 训练得到出方法论
4. 在测试集上使用方法论

**监督学习有2个主要的任务：**
1. 回归：预测连续的、具体的数值。
2. 分类：对各种事物分门别类，用于离散型预测。

**主流监督学习算法**
|算法|类型|简介|
|--|--|--|
|朴素贝叶斯|	分类|	贝叶斯分类法是基于贝叶斯定定理的统计学分类方法。它通过预测一个给定的元组属于一个特定类的概率，来进行分类。朴素贝叶斯分类法假定一个属性值在给定类的影响独立于其他属性的 —— 类条件独立性。|
|决策树|	分类|	决策树是一种简单但广泛使用的分类器，它通过训练数据构建决策树，对未知的数据进行分类。|
|SVM|	分类|	支持向量机把分类问题转化为寻找分类平面的问题，并通过最大化分类边界点距离分类平面的距离来实现分类。|
|逻辑回归|	分类|	逻辑回归是用于处理因变量为分类变量的回归问题，常见的是二分类或二项分布问题，也可以处理多分类问题，它实际上是属于一种分类方法。|
|线性回归|	回归|	线性回归是处理回归任务最常用的算法之一。该算法的形式十分简单，它期望使用一个超平面拟合数据集（只有两个变量的时候就是一条直线）。|
|回归树|	回归|	回归树（决策树的一种）通过将数据集重复分割为不同的分支而实现分层学习，分割的标准是最大化每一次分离的信息增益。这种分支结构让回归树很自然地学习到非线性关系。|
|K邻近|	分类+回归|	通过搜索K个最相似的实例（邻居）的整个训练集并总结那些K个实例的输出变量，对新数据点进行预测。|
|Adaboosting|	分类+回归|	Adaboost目的就是从训练数据中学习一系列的弱分类器或基本分类器，然后将这些弱分类器组合成一个强分类器。|
|神经网络|	分类+回归|	它从信息处理角度对人脑神经元网络进行抽象， 建立某种简单模型，按不同的连接方式组成不同的网络。|

### 1.2. 无监督学习
无监督学习本质上是一个统计手段，在没有标签的数据里可以发现潜在的一些结构的一种训练方式。它具有三个特点：
1. **无监督学习没有明确目的的训练方式，你无法提前知道结果是什么。**
2. **无监督学习不需要给数据打标签。**
3. **无监督学习几乎无法量化效果如何。**


**常见的2类算法是**：
|类别|算法|j简介|方法|
|--|--|--|--|
|聚类|K均值聚类|制定分组的数量为K，自动进行分组|1. 定义 K 个重心。一开始这些重心是随机的（也有一些更加有效的用于初始化重心的算法）<br>2. 寻找最近的重心并且更新聚类分配。将每个数据点都分配给这 K 个聚类中的一个。每个数据点都被分配给离它们最近的重心的聚类。这里的「接近程度」的度量是一个超参数——通常是欧几里得距离（Euclidean distance）。<br>3. 将重心移动到它们的聚类的中心。每个聚类的重心的新位置是通过计算该聚类中所有数据点的平均位置得到的。<br>重复2、3步，直到该算法收敛|
|聚类|层次聚类|不知道应该分为几类，使用层次聚类|1. 首先从 N 个聚类开始，每个数据点一个聚类。<br>2. 将彼此靠得最近的两个聚类融合为一个。现在你有 N-1 个聚类。<br>3. 重新计算这些聚类之间的距离。<br>4. 重复第 2 和 3 步，直到你得到包含 N 个数据点的一个聚类。<br>5. 选择一个聚类数量，然后在这个树状图中划一条水平线。
|降维|主成分分析 – PCA|主成分分析是把多指标转化为少数几个综合指标|1. 第一步计算矩阵 X 的样本的协方差矩阵 S（此为不标准PCA，标准PCA计算相关系数矩阵C）<br>2. 第二步计算协方差矩阵S（或C）的特征向量 e1,e2,…,eN和特征值 , t = 1,2,…,N<br>3. 第三步投影数据到特征向量张成的空间之中。利用下面公式，其中BV值是原样本中对应维度的值。<br>$newBV_{i,p}=\sum^{n}_{k=1}e_iBV_{i,p}$
|降维|奇异值分解 – SVD|奇异值分解（Singular Value Decomposition）是线性代数中一种重要的矩阵分解，奇异值分解则是特征分解在任意矩阵上的推广。在信号处理、统计学等领域有重要应用。|
## 2. 梯度下降
## 3. 正则项
## 4. 损失函数
## 5. 参数与非参数模型
参数模型的前提是假设数据服从某一分布，该分布由一些参数确定（比如正太分布由均值和方差确定），在此基础上构建的模型称为参数模型；  
非参数模型对于总体的分布不做任何假设，只是知道总体是一个随机变量，其分布是存在的（分布中也可能存在参数），但是无法知道其分布的形式，更不知道分布的相关参数，只有在给定一些样本的条件下，能够依据非参数统计的方法进行推断。