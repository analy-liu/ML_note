 # python 爬虫
- [python 爬虫](#python-爬虫)
  - [1. python爬虫包的介绍与使用](#1-python爬虫包的介绍与使用)
    - [1.1. requests](#11-requests)
      - [1.1.1. requests.get](#111-requestsget)
        - [1.1.1.1. requests.get参数设置](#1111-requestsget参数设置)
        - [1.1.1.2. requests.get返回值](#1112-requestsget返回值)
    - [1.2. lxml](#12-lxml)
      - [1.2.1. lxml.etree](#121-lxmletree)
        - [1.2.1.1. 使用xpath获取html文件目标值语法](#1211-使用xpath获取html文件目标值语法)
        - [1.2.1.2. 使用xpath获取html文件目标值例子](#1212-使用xpath获取html文件目标值例子)
    - [1.3. selenium](#13-selenium)
  - [2. 解析网页技巧](#2-解析网页技巧)
    - [2.1. 突破前端反调试--阻止页面不断debugger](#21-突破前端反调试--阻止页面不断debugger)
    - [2.2. 破解反爬虫技巧](#22-破解反爬虫技巧)
      - [2.2.1. Headers反爬虫](#221-headers反爬虫)
      - [2.2.2. IP限制](#222-ip限制)
      - [2.2.3. User-Agent限制](#223-user-agent限制)
## 1. python爬虫包的介绍与使用
### 1.1. requests
requests是一个Python第三方库，用于访问网络资源，处理URL资源特别方便  
安装 ：

    pip install requests
#### 1.1.1. requests.get
##### 1.1.1.1. requests.get参数设置
```python
requests.get(url: str | bytes, params: SupportsItems | Tuple | Iterable | str | bytes | None = ..., **kwargs) -> Response

# 简单使用(只使用url参数)
target_url = 'https://github.com/'
r = requests.get(target_url)

# 进阶使用(使用params参数)
payload = {'key1': 'value1', 'key2': 'value2'}
r = requests.get("'https://www.google.com.hk/search", params=payload)
# r.url的输出为：
# 'https://www.google.com.hk/search?key2=value2&key1=value1'

# 详细使用(使用**kwargs)
proxy = {
    'https': 'https://127.0.0.1:1080',
    'http': 'http://127.0.0.1:1080'
}
header = {
    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.150 Safari/537.36 Edg/88.0.705.68'
## 设置头部信息，代理ip，不使用证书
r = requests.get(target_url, verify=False, headers=header, proxies=proxy) 
```
##### 1.1.1.2. requests.get返回值
返回值通常命名为 r 或 response
|代码|描述|
|:-|:-|
|**r.status_code**|响应状态码|
|**r.raw**|原始响应体，使用r.raw.read()读取|
|**r.content**|字节方式的响应体，需要进行解码|
|**r.content.decode**|("编码格式") 当编码格式与响应头部的字符编码相同时，内容与r.text|
|**r.text**|字符串方式的响应体，会自动更具响应头部的字符编码进行解码|
|**r.headers**|以字典对象储存服务器响应头，但是这个字典比较特殊，字典键不区分大小写，若键不存在，则返回None|
|**r.json()**|request中内置的json解码器|
|**r.raise_for_status()**|请求失败(非200响应)，抛出异常|
|**r.url**|获取请求的url|
|**r.cookies**|获取请求后的cookies|
|**r.encoding**|获取编码格式|
### 1.2. lxml
lxml是一个Python库，使用它可以轻松处理XML和HTML文件，还可以用于web爬取。  
卸载：

    pip uninstall lxml  
安装：

    python -m pip install lxml -i https://pypi.tuna.tsinghua.edu.cn/simple
#### 1.2.1. lxml.etree
导入

    from lxml import etree
##### 1.2.1.1. 使用xpath获取html文件目标值语法
```python
# 根据html字符串建立html_xml, 这里使用requests.get的返回值
html_xml = etree.HTML(r.text)
target_element = html_xml.xpath('xpath')
```
##### 1.2.1.2. 使用xpath获取html文件目标值例子
案例html：
```html
<!DOCTYPE html>\n<html lang="zh-CN">
<head>
</head>
<body>
    <ul id="menu" class="menu">
        <li><a title="1" href="https://www.1.com">一</a></li>
        <li><a title="2" href="https://www.2.com">二</a></li>
        <li><a title="3" href="https://www.3.com">三</a></li>
    </ul>
    <ul id="text" class="text">
        <li><a title="1" href="https://www.text1.com">text一</a></li>
        <li><a title="2" href="https://www.text2.com">text二</a></li>
        <li><a title="3" href="https://www.text3.com">text三</a></li>
    </ul>
</body>
</html>
```
```python
#获取id="menu"的ul中的所有href
target_element = html_xml.xpath('\\ul[@id="menu"\li\a\@href')
# 返回
["https://www.1.com"，"https://www.2.com"，"https://www.3.com"]

#获取id="text"的ul中的倒数第二个文本内容
target_element = html_xml.xpath('\\ul[@id="text"\li[last()-1]\a\text()')
# 返回
["text二"]
```
### 1.3. selenium

## 2. 解析网页技巧
### 2.1. 突破前端反调试--阻止页面不断debugger
在debugger处添加条件断点，条件为false
参考网站：https://segmentfault.com/a/1190000012359015

### 2.2. 破解反爬虫技巧
#### 2.2.1. Headers反爬虫
   1. 检查: Cookie、Referer、User-Agent
   2. 解决方案: 通过F12获取headers,传给requests.get()方法
        
#### 2.2.2. IP限制
   1. 网站根据IP地址访问频率进行反爬,短时间内限制IP访问
   2. 解决方案: 
        a. 构造自己IP代理池,每次访问随机选择代理,经常更新代理池
        b. 购买开放代理或私密代理IP
        c. 降低爬取的速度
        
#### 2.2.3. User-Agent限制
   1. 类似于IP限制，检测频率
   2. 解决方案: 构造自己的User-Agent池,每次访问随机选择
        a. fake_useragent模块
        b. 新建py文件,存放大量User-Agent