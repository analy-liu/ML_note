 # 机器学习流程（二）----基础机器学习算法
本篇介绍各类基础机器学习算法的简介，以及算法的优缺点
算法实现或详细原理与运用即调参，见具体算法文档
- [机器学习流程（二）----基础机器学习算法](#机器学习流程二----基础机器学习算法)
  - [1. 监督学习](#1-监督学习)
    - [1.1. 回归（Regression）](#11-回归regression)
      - [1.1.1. 线性回归](#111-线性回归)
      - [1.1.2. 岭回归](#112-岭回归)
    - [1.2. 分类（Classification）](#12-分类classification)
    - [1.3. 分类与回归](#13-分类与回归)
  - [2. 非监督学习](#2-非监督学习)
    - [2.1. 聚类（Clustering）](#21-聚类clustering)
    - [2.2. 降维（Dimensionality Reduction）](#22-降维dimensionality-reduction)

机器学习算法大致分为监督学习与非监督学习
## 1. 监督学习
监督学习是一种目的明确的训练方式，数据集是有标签的。
监督学习的流程：
1. 选择一个适合目标任务的机器学习模型
2. 把训练集给机器去学习（监督学习需要给数据打标签）
3. 训练得到出方法论
4. 在测试集上使用方法论

### 1.1. 回归（Regression）
回归是一种用于连续型数值变量预测和建模的监督学习算法。
回归算法有很多，这些算法的不同主要在于三方面：
1. 自变量的个数：一个or多个
2. 因变量的类型：连续or离散
3. 回归线的形状：线性or非线性

对于那些有创意的人，如果你觉得有必要使用上面这些参数的一个组合，你甚至可以创造出一个没有被使用过的回归模型。

常用的回归算法有：线性回归、逻辑回归、多项式回归、逐步回归、岭回归、套索回归、ElasticNet回归、回归树

#### 1.1.1. 线性回归
**简介**
线性回归中，因变量是连续的，自变量只有一个，回归线的性质是线性的。
用一个方程表示，即：$y = a + bx + e$  
a表示截距，b表示斜率，e是误差项  

**拟合方法**
线性回归的拟合方法为**最小二乘法**
最小二乘法：实际值到回归线的垂直偏差的平方和最小，即$\min(\sum (y_i-\hat{y_i})^2)$

**优缺点**
优点：
1. 速度快
2. 可解释性好

缺点：
1. 不适合非线性

#### 1.1.2. 岭回归
### 1.2. 分类（Classification）
常用的分类算法有：Logistic 回归（LR）、线性判别分析算法（LDA）、朴素贝叶斯、支持向量机SVM、决策树、随机森林、XGBoost
### 1.3. 分类与回归
既能解决分类又能解决回归问题的算法有：Adaboosting、KNN、学习向量量化算法（LVQ）、神经网络
## 2. 非监督学习
无监督学习本质上是一个统计手段，在没有标签的数据里可以发现潜在的一些结构的一种训练方式。它具有三个特点：
1. **无监督学习没有明确目的的训练方式，你无法提前知道结果是什么。**
2. **无监督学习不需要给数据打标签。**
3. **无监督学习几乎无法量化效果如何。**
### 2.1. 聚类（Clustering）
常见的聚类算法有：K均值聚类、层次聚类
### 2.2. 降维（Dimensionality Reduction）
常见的降维算法有：主成分分析法（PAC）、奇异值分解（SVD）

