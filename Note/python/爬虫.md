 # python 爬虫
- [python 爬虫](#python-爬虫)
  - [1. python爬虫包的介绍与使用](#1-python爬虫包的介绍与使用)
    - [1.1. requests](#11-requests)
      - [1.1.1. requests.get](#111-requestsget)
        - [1.1.1.1. requests.get参数设置](#1111-requestsget参数设置)
        - [1.1.1.2. requests.get返回值](#1112-requestsget返回值)
      - [1.1.2. requests.post](#112-requestspost)
        - [1.1.2.1. requests.post参数设置](#1121-requestspost参数设置)
        - [1.1.2.2. requests.post返回值](#1122-requestspost返回值)
    - [1.2. lxml](#12-lxml)
      - [1.2.1. lxml.etree](#121-lxmletree)
        - [1.2.1.1. 使用xpath获取html文件目标值语法](#1211-使用xpath获取html文件目标值语法)
        - [1.2.1.2. 使用xpath获取html文件目标值例子](#1212-使用xpath获取html文件目标值例子)
    - [1.3. selenium](#13-selenium)
      - [基本操作](#基本操作)
      - [页面等待](#页面等待)
      - [警告框处理](#警告框处理)
      - [下拉框选择](#下拉框选择)
      - [cookie操作](#cookie操作)
  - [2. 解析网页技巧](#2-解析网页技巧)
    - [2.1. 突破前端反调试--阻止页面不断debugger](#21-突破前端反调试--阻止页面不断debugger)
    - [2.2. 破解反爬虫技巧](#22-破解反爬虫技巧)
      - [2.2.1. Headers反爬虫](#221-headers反爬虫)
      - [2.2.2. IP限制](#222-ip限制)
      - [2.2.3. User-Agent限制](#223-user-agent限制)
## 1. python爬虫包的介绍与使用
### 1.1. requests
requests是一个Python第三方库，用于访问网络资源，处理URL资源特别方便  
安装 ：

    pip install requests
#### 1.1.1. requests.get
##### 1.1.1.1. requests.get参数设置
```python
requests.get(url: str | bytes, params: SupportsItems | Tuple | Iterable | str | bytes | None = ..., **kwargs) -> Response

# 简单使用(只使用url参数)
target_url = 'https://github.com/'
r = requests.get(target_url)

# 进阶使用(使用params参数)
payload = {'key1': 'value1', 'key2': 'value2'}
r = requests.get("'https://www.google.com.hk/search", params=payload)
# r.url的输出为：
# 'https://www.google.com.hk/search?key2=value2&key1=value1'

# 详细使用(使用**kwargs)
proxy = {
    'https': 'https://127.0.0.1:1080',
    'http': 'http://127.0.0.1:1080'
}
header = {
    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.150 Safari/537.36 Edg/88.0.705.68'
## 设置头部信息，代理ip，不使用证书
r = requests.get(target_url, verify=False, headers=header, proxies=proxy) 
```
##### 1.1.1.2. requests.get返回值
返回值通常命名为 r 或 response
|代码|描述|
|:-|:-|
|**r.status_code**|响应状态码|
|**r.raw**|原始响应体，使用r.raw.read()读取|
|**r.content**|字节方式的响应体，需要进行解码|
|**r.content.decode**|("编码格式") 当编码格式与响应头部的字符编码相同时，内容与r.text|
|**r.text**|字符串方式的响应体，会自动更具响应头部的字符编码进行解码|
|**r.headers**|以字典对象储存服务器响应头，但是这个字典比较特殊，字典键不区分大小写，若键不存在，则返回None|
|**r.json()**|request中内置的json解码器|
|**r.raise_for_status()**|请求失败(非200响应)，抛出异常|
|**r.url**|获取请求的url|
|**r.cookies**|获取请求后的cookies|
|**r.encoding**|获取编码格式|

#### 1.1.2. requests.post
##### 1.1.2.1. requests.post参数设置
```
requests.post(url, data=None, json=None, **kwargs)
```
url: 必填  

data与json  根据请求头中的Content-Type情况选择一个输入  
1. Content-Type: application/x-www-form-urlencoded;
使用data，传入dict
2. Content-Type: application/json
使用json，传入str

```python
import json
# dict转json
json_data = json.dumps(dict_data)
# json转dict
dict_data = json.loads(json_data)
```
##### 1.1.2.2. requests.post返回值
同requests.get
### 1.2. lxml
lxml是一个Python库，使用它可以轻松处理XML和HTML文件，还可以用于web爬取。  
卸载：

    pip uninstall lxml  
安装：

    python -m pip install lxml -i https://pypi.tuna.tsinghua.edu.cn/simple
#### 1.2.1. lxml.etree
导入

    from lxml import etree
##### 1.2.1.1. 使用xpath获取html文件目标值语法
```python
# 根据html字符串建立html_xml, 这里使用requests.get的返回值
html_xml = etree.HTML(r.text)
target_element = html_xml.xpath('xpath')
```
##### 1.2.1.2. 使用xpath获取html文件目标值例子
案例html：
```html
<!DOCTYPE html>\n<html lang="zh-CN">
<head>
</head>
<body>
    <ul id="menu" class="menu">
        <li><a title="1" href="https://www.1.com">一</a></li>
        <li><a title="2" href="https://www.2.com">二</a></li>
        <li><a title="3" href="https://www.3.com">三</a></li>
    </ul>
    <ul id="text" class="text">
        <li><a title="1" href="https://www.text1.com">text一</a></li>
        <li><a title="2" href="https://www.text2.com">text二</a></li>
        <li><a title="3" href="https://www.text3.com">text三</a></li>
    </ul>
</body>
</html>
```
```python
#获取id="menu"的ul中的所有href
target_element = html_xml.xpath('\\ul[@id="menu"\li\a\@href')
# 返回
["https://www.1.com"，"https://www.2.com"，"https://www.3.com"]

#获取id="text"的ul中的倒数第二个文本内容
target_element = html_xml.xpath('\\ul[@id="text"\li[last()-1]\a\text()')
# 返回
["text二"]
```
### 1.3. selenium
准备工作：
1. 安装：pip install selenium
2. 下载浏览器驱动
  Firefox浏览器驱动：[geckodriver](https://github.com/mozilla/geckodriver/releases)
  Chrome浏览器驱动：[chromedriver](https://sites.google.com/a/chromium.org/chromedriver/home)
  IE浏览器驱动：[IEDriverServer](http://selenium-release.storage.googleapis.com/index.html)
  Edge浏览器驱动：[MicrosoftWebDriver](https://developer.microsoft.com/en-us/microsoft-edge/tools/webdriver)
  Opera浏览器驱动：[operadriver](https://github.com/operasoftware/operachromiumdriver/releases)
  PhantomJS浏览器驱动：[phantomjs](http://phantomjs.org/)
3. 设置环境变量
#### 基本操作
```python
from selenium import webdriver
# 打开浏览器
driver = webdriver.Edge(executable_path='MicrosoftWebDriver位置')

# 打开目标网页
target_url = ""
driver.get(target_url)

# 页面操作
driver.back()# 后退 
driver.forward()# 前进 
driver.refresh() # 刷新
driver.save_screenshot("保存位置")# 页面截图

# 寻找元素
element = driver.find_element_by_xpath("")
# function
find_element_by_id()
find_element_by_name()
find_element_by_class_name()
find_element_by_tag_name()
find_element_by_link_text()
find_element_by_partial_link_text()
find_element_by_xpath()
find_element_by_css_selector()

# 元素操作
element.click()# 点击元素
element.clear() # 清除文本
element.send_keys('')# 在元素中输入
element.screenshot('保存位置')# 元素截图
element.send_keys('').submit()# 输入并回车提交
element.send_keys('D:\\upload_file.txt')# 文件上传

# 元素判断
element.is_displayed()# 判断元素用户是否可见
element.get_attribute(name)# 获得属性值
element.text # 获取元素的文本
element.size # 获取元素的尺寸

# 键盘操作
send_keys(Keys.BACK_SPACE) 删除键（BackSpace）
send_keys(Keys.SPACE) 空格键(Space)
send_keys(Keys.TAB) 制表键(Tab)
send_keys(Keys.ESCAPE) 回退键（Esc）
send_keys(Keys.ENTER) 回车键（Enter）
send_keys(Keys.CONTROL,'a') 全选（Ctrl+A）
send_keys(Keys.CONTROL,'c') 复制（Ctrl+C）
send_keys(Keys.CONTROL,'x') 剪切（Ctrl+X）
send_keys(Keys.CONTROL,'v') 粘贴（Ctrl+V）
send_keys(Keys.F1) 键盘 F1
……
send_keys(Keys.F12) 键盘 F12

# 鼠标操作
# 引入 ActionChains 类
from selenium.webdriver.common.action_chains import ActionChains
element = driver.find_element_by_link_text("设置")# 设置元素
ActionChains(driver).move_to_element(element).perform()# 悬停操作
# function
perform()： 执行所有 ActionChains 中存储的行为；
context_click()： 右击；
double_click()： 双击；
drag_and_drop()： 拖动；
move_to_element()： 鼠标悬停。

# 关闭浏览器
driver.close()
```
#### 页面等待
```python
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
element = WebDriverWait(driver, 5, 0.5).until(
                      EC.presence_of_element_located((By.ID, "kw"))
                      )
```
**function**
WebDriverWait(driver, timeout, poll_frequency=0.5, ignored_exceptions=None)
driver ：浏览器驱动。
timeout ：最长超时时间，默认以秒为单位。
poll_frequency ：检测的间隔（步长）时间，默认为0.5S。
ignored_exceptions ：超时后的异常信息，默认情况下抛NoSuchElementException异常。

WebDriverWait()一般由until()或until_not()方法配合使用，下面是until()和until_not()方法的说明。
until(method, message=‘’) 调用该方法提供的驱动程序作为一个参数，直到返回值为True。
until_not(method, message=‘’) 调用该方法提供的驱动程序作为一个参数，直到返回值为False。

#### 警告框处理
```python
alert = driver.switch_to_alert()
```
text：返回 alert/confirm/prompt 中的文字信息。
accept()：接受现有警告框。
dismiss()：解散现有警告框。
send_keys(keysToSend)：发送文本至警告框。keysToSend：将文本发送至警告框。
#### 下拉框选择
```python
from selenium import webdriver
from selenium.webdriver.support.select import Select
from time import sleep

driver = webdriver.Chrome()
driver.implicitly_wait(10)
driver.get('http://www.baidu.com')
sel = driver.find_element_by_xpath("//select[@id='nr']")
Select(sel).select_by_value('50')  # 显示50条
```
#### cookie操作
```python
#WebDriver操作cookie的方法：

driver.get_cookies() #获得所有cookie信息。
driver.get_cookie(name)# 返回字典的key为“name”的cookie信息。
driver.add_cookie(cookie_dict) # 添加cookie。“cookie_dict”指字典对象，必须有name 和value 值。
driver.delete_cookie(name,optionsString)# 删除cookie信息。“name”是要删除的driver.cookie的名称，“optionsString”是该cookie的选项，目前支持的选项包括“路径”，“域”。
driver.delete_all_cookies()# 删除所有cookie信息
```
## 2. 解析网页技巧
### 2.1. 突破前端反调试--阻止页面不断debugger
在debugger处添加条件断点，条件为false
参考网站：https://segmentfault.com/a/1190000012359015

### 2.2. 破解反爬虫技巧
#### 2.2.1. Headers反爬虫
   1. 检查: Cookie、Referer、User-Agent
   2. 解决方案: 通过F12获取headers,传给requests.get()方法
        
#### 2.2.2. IP限制
   1. 网站根据IP地址访问频率进行反爬,短时间内限制IP访问
   2. 解决方案: 
        a. 构造自己IP代理池,每次访问随机选择代理,经常更新代理池
        b. 购买开放代理或私密代理IP
        c. 降低爬取的速度
        
#### 2.2.3. User-Agent限制
   1. 类似于IP限制，检测频率
   2. 解决方案: 构造自己的User-Agent池,每次访问随机选择
        a. fake_useragent模块
        b. 新建py文件,存放大量User-Agent